{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Download and extract dataset\n",
    "def download_and_extract_dataset():\n",
    "    # Download face mask classification dataset\n",
    "    url = \"https://github.com/chandrikadeb7/Face-Mask-Detection/archive/refs/heads/master.zip\"\n",
    "    print(\"Downloading face mask dataset...\")\n",
    "    r = requests.get(url, stream=True)\n",
    "    with zipfile.ZipFile(BytesIO(r.content)) as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "    \n",
    "    # Create directories for processed data\n",
    "    os.makedirs(\"data/with_mask\", exist_ok=True)\n",
    "    os.makedirs(\"data/without_mask\", exist_ok=True)\n",
    "    \n",
    "    # Copy files to our working directory\n",
    "    source_with_mask = \"./Face-Mask-Detection-master/dataset/with_mask\"\n",
    "    source_without_mask = \"./Face-Mask-Detection-master/dataset/without_mask\"\n",
    "    \n",
    "    for filename in os.listdir(source_with_mask):\n",
    "        src_path = os.path.join(source_with_mask, filename)\n",
    "        dst_path = os.path.join(\"data/with_mask\", filename)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    for filename in os.listdir(source_without_mask):\n",
    "        src_path = os.path.join(source_without_mask, filename)\n",
    "        dst_path = os.path.join(\"data/without_mask\", filename)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "    \n",
    "    print(\"Dataset downloaded and extracted successfully.\")\n",
    "\n",
    "# Run the download function\n",
    "download_and_extract_dataset()\n",
    "\n",
    "# Load and preprocess images\n",
    "def load_images(with_mask_dir, without_mask_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load with_mask images\n",
    "    for filename in tqdm(os.listdir(with_mask_dir), desc=\"Loading with_mask images\"):\n",
    "        img_path = os.path.join(with_mask_dir, filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (64, 64))  # Resize for consistency\n",
    "            images.append(img)\n",
    "            labels.append(1)  # 1 for with_mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    # Load without_mask images\n",
    "    for filename in tqdm(os.listdir(without_mask_dir), desc=\"Loading without_mask images\"):\n",
    "        img_path = os.path.join(without_mask_dir, filename)\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (64, 64))  # Resize for consistency\n",
    "            images.append(img)\n",
    "            labels.append(0)  # 0 for without_mask\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Load dataset\n",
    "images, labels = load_images(\"data/with_mask\", \"data/without_mask\")\n",
    "\n",
    "# Display some sample images\n",
    "def plot_sample_images(images, labels, num_samples=5):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    mask_samples = np.where(labels == 1)[0][:num_samples]\n",
    "    no_mask_samples = np.where(labels == 0)[0][:num_samples]\n",
    "    \n",
    "    for i, idx in enumerate(mask_samples):\n",
    "        plt.subplot(2, num_samples, i + 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.title(\"With Mask\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    for i, idx in enumerate(no_mask_samples):\n",
    "        plt.subplot(2, num_samples, i + num_samples + 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.title(\"Without Mask\")\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_sample_images(images, labels)\n",
    "\n",
    "# Feature extraction functions\n",
    "def extract_histogram_features(img):\n",
    "    \"\"\"Extract color histogram features\"\"\"\n",
    "    features = []\n",
    "    for channel in range(3):  # RGB channels\n",
    "        hist = cv2.calcHist([img], [channel], None, [32], [0, 256])\n",
    "        features.extend(hist.flatten())\n",
    "    return features\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    \"\"\"Extract HOG (Histogram of Oriented Gradients) features\"\"\"\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    h, w = gray.shape\n",
    "    \n",
    "    # Parameters for HOG\n",
    "    cell_size = (8, 8)\n",
    "    block_size = (2, 2)\n",
    "    nbins = 9\n",
    "    \n",
    "    # Calculate gradient\n",
    "    gx = cv2.Sobel(gray, cv2.CV_32F, 1, 0, ksize=1)\n",
    "    gy = cv2.Sobel(gray, cv2.CV_32F, 0, 1, ksize=1)\n",
    "    \n",
    "    # Calculate gradient magnitude and orientation\n",
    "    magnitude, orientation = cv2.cartToPolar(gx, gy, angleInDegrees=True)\n",
    "    \n",
    "    # Quantize orientations into bins\n",
    "    orientation = orientation / 180 * nbins\n",
    "    \n",
    "    # Simple HOG implementation for feature extraction\n",
    "    hog_features = []\n",
    "    for i in range(0, h, cell_size[0]):\n",
    "        for j in range(0, w, cell_size[1]):\n",
    "            if i + cell_size[0] <= h and j + cell_size[1] <= w:\n",
    "                cell_magnitude = magnitude[i:i+cell_size[0], j:j+cell_size[1]]\n",
    "                cell_orientation = orientation[i:i+cell_size[0], j:j+cell_size[1]]\n",
    "                \n",
    "                hist = np.zeros(nbins)\n",
    "                for o_idx in range(nbins):\n",
    "                    # Find pixels with orientations in the current bin\n",
    "                    mask = ((cell_orientation >= o_idx) & (cell_orientation < (o_idx + 1)))\n",
    "                    hist[o_idx] = np.sum(cell_magnitude[mask])\n",
    "                \n",
    "                hog_features.extend(hist)\n",
    "    \n",
    "    return hog_features\n",
    "\n",
    "def extract_features(images):\n",
    "    \"\"\"Extract combined features from images\"\"\"\n",
    "    all_features = []\n",
    "    for img in tqdm(images, desc=\"Extracting features\"):\n",
    "        # Get histogram features\n",
    "        hist_features = extract_histogram_features(img)\n",
    "        \n",
    "        # Get HOG features\n",
    "        hog_features = extract_hog_features(img)\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = np.concatenate([hist_features, hog_features])\n",
    "        all_features.append(combined_features)\n",
    "    \n",
    "    return np.array(all_features)\n",
    "\n",
    "# Extract features from all images\n",
    "features = extract_features(images)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train and evaluate SVM classifier\n",
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    print(\"Training SVM classifier...\")\n",
    "    svm = SVC(kernel='rbf', C=10, gamma='scale', probability=True)\n",
    "    svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"SVM Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Without Mask', 'With Mask']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Without Mask', 'With Mask'],\n",
    "                yticklabels=['Without Mask', 'With Mask'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - SVM')\n",
    "    plt.show()\n",
    "    \n",
    "    return svm, accuracy\n",
    "\n",
    "# Train and evaluate Neural Network classifier\n",
    "def train_nn(X_train, y_train, X_test, y_test):\n",
    "    print(\"Training Neural Network classifier...\")\n",
    "    nn = MLPClassifier(hidden_layer_sizes=(100, 50), \n",
    "                       activation='relu', \n",
    "                       solver='adam',\n",
    "                       alpha=0.0001,\n",
    "                       max_iter=300,\n",
    "                       random_state=42)\n",
    "    \n",
    "    nn.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    y_pred = nn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Neural Network Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Without Mask', 'With Mask']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Without Mask', 'With Mask'],\n",
    "                yticklabels=['Without Mask', 'With Mask'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix - Neural Network')\n",
    "    plt.show()\n",
    "    \n",
    "    return nn, accuracy\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "svm_model, svm_accuracy = train_svm(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "nn_model, nn_accuracy = train_nn(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "# Compare classifier performances\n",
    "def compare_classifiers(classifiers, accuracies):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(classifiers, accuracies, color=['blue', 'green'])\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2 - 0.1, \n",
    "                 bar.get_height() + 0.01, \n",
    "                 f'{acc:.4f}', \n",
    "                 fontsize=12)\n",
    "    \n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title('Classifier Accuracy Comparison (Handcrafted Features)')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "compare_classifiers(['SVM', 'Neural Network'], [svm_accuracy, nn_accuracy])\n",
    "\n",
    "\n",
    "# Define a custom dataset class\n",
    "class FaceMaskDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Define data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_img, X_test_img, y_train_img, y_test_img = train_test_split(\n",
    "    images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = FaceMaskDataset(X_train_img, y_train_img, transform=transform)\n",
    "test_dataset = FaceMaskDataset(X_test_img, y_test_img, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define CNN model\n",
    "class FaceMaskCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceMaskCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First convolutional block\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Second convolutional block\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Third convolutional block\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # Flatten the output\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Function to train the model\n",
    "def train_cnn(model, train_loader, test_loader, criterion, optimizer, num_epochs=10, device=\"cpu\"):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        epoch_test_loss = running_loss / len(test_loader.dataset)\n",
    "        test_losses.append(epoch_test_loss)\n",
    "        \n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {epoch_train_loss:.4f}, Test Loss: {epoch_test_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return train_losses, test_losses, test_accuracies, all_preds, all_labels\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_training_history(train_losses, test_losses, test_accuracies):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Testing Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(test_accuracies)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Test Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize model\n",
    "model = FaceMaskCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "train_losses, test_losses, test_accuracies, y_pred_cnn, y_test_cnn = train_cnn(\n",
    "    model, train_loader, test_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(train_losses, test_losses, test_accuracies)\n",
    "\n",
    "# Evaluation\n",
    "print(\"CNN Final Accuracy:\", test_accuracies[-1])\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_cnn, y_pred_cnn, target_names=['Without Mask', 'With Mask']))\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test_cnn, y_pred_cnn)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Without Mask', 'With Mask'],\n",
    "            yticklabels=['Without Mask', 'With Mask'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - CNN')\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter experimentation\n",
    "def cnn_hyperparameter_experiment():\n",
    "    # Define hyperparameter combinations to try\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "    optimizers = [\n",
    "        ('SGD', optim.SGD),\n",
    "        ('Adam', optim.Adam)\n",
    "    ]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "    \n",
    "    # Dictionary to store results\n",
    "    results = []\n",
    "    \n",
    "    # Iterate through hyperparameter combinations\n",
    "    for lr in learning_rates:\n",
    "        for opt_name, opt_class in optimizers:\n",
    "            for bs in batch_sizes:\n",
    "                print(f\"\\nTesting hyperparameters: LR={lr}, Optimizer={opt_name}, Batch Size={bs}\")\n",
    "                \n",
    "                # Create data loaders with the current batch size\n",
    "                train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "                test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "                \n",
    "                # Initialize model\n",
    "                model = FaceMaskCNN()\n",
    "                \n",
    "                # Define optimizer with current learning rate\n",
    "                if opt_name == 'SGD':\n",
    "                    optimizer = opt_class(model.parameters(), lr=lr, momentum=0.9)\n",
    "                else:\n",
    "                    optimizer = opt_class(model.parameters(), lr=lr)\n",
    "                \n",
    "                # Train for fewer epochs to save time\n",
    "                num_epochs = 5\n",
    "                _, _, test_accuracies, _, _ = train_cnn(\n",
    "                    model, train_loader, test_loader, criterion, optimizer, num_epochs, device)\n",
    "                \n",
    "                # Store the final accuracy\n",
    "                final_accuracy = test_accuracies[-1]\n",
    "                results.append({\n",
    "                    'learning_rate': lr,\n",
    "                    'optimizer': opt_name,\n",
    "                    'batch_size': bs,\n",
    "                    'accuracy': final_accuracy\n",
    "                })\n",
    "                \n",
    "                print(f\"Final accuracy: {final_accuracy:.4f}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Find the best hyperparameter combination\n",
    "    best_idx = results_df['accuracy'].idxmax()\n",
    "    best_params = results_df.iloc[best_idx]\n",
    "    \n",
    "    print(\"\\nBest Hyperparameter Combination:\")\n",
    "    print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "    print(f\"Optimizer: {best_params['optimizer']}\")\n",
    "    print(f\"Batch Size: {best_params['batch_size']}\")\n",
    "    print(f\"Accuracy: {best_params['accuracy']:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, opt in enumerate(['SGD', 'Adam']):\n",
    "        plt.subplot(2, 1, i+1)\n",
    "        for bs in batch_sizes:\n",
    "            df_subset = results_df[(results_df['optimizer'] == opt) & (results_df['batch_size'] == bs)]\n",
    "            plt.plot(df_subset['learning_rate'], df_subset['accuracy'], \n",
    "                     marker='o', label=f'Batch Size={bs}')\n",
    "        \n",
    "        plt.xscale('log')\n",
    "        plt.xlabel('Learning Rate')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title(f'Hyperparameter Tuning - {opt}')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df, best_params\n",
    "\n",
    "# Run hyperparameter experiment\n",
    "results_df, best_params = cnn_hyperparameter_experiment()\n",
    "\n",
    "# Compare all classifier performances (handcrafted+ML vs CNN)\n",
    "def compare_all_classifiers(classifiers, accuracies):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    bars = plt.bar(classifiers, accuracies, color=colors)\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2 - 0.1, \n",
    "                 bar.get_height() + 0.01, \n",
    "                 f'{acc:.4f}', \n",
    "                 fontsize=12)\n",
    "    \n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title('Classifier Accuracy Comparison')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "# Compare SVM, NN, and CNN performances\n",
    "compare_all_classifiers(['SVM (Handcrafted)', 'NN (Handcrafted)', 'CNN'], \n",
    "                        [svm_accuracy, nn_accuracy, test_accuracies[-1]])\n",
    "\n",
    "# Save best models for later use\n",
    "torch.save(model.state_dict(), 'cnn_face_mask_model.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
